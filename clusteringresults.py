# -*- coding: utf-8 -*-
"""clusteringresults.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dAmhw7_n4mgLp83KowP4_RLOBi_JtPGk
"""

import pandas as pd

df = pd.read_excel("/content/1dataset.xlsx")
display(df.head())

"""# Task
Run k-means clustering on the data in "/content/1dataset.xlsx" to group individuals based on their training needs, and analyze the characteristics of each cluster in terms of age, gender, and years of experience in teaching.

## Select features

### Subtask:
Choose the relevant columns for clustering (training needs).

**Reasoning**:
Identify columns related to training needs based on their naming convention and create a new DataFrame with these columns.
"""

features_df = df.filter(regex=r'^\d+\.\d+\.\s')
display(features_df.head())

"""**Reasoning**:
Check for missing values in the selected features to ensure data quality before clustering.


"""

missing_values = features_df.isnull().sum()
print("Missing values per column:")
print(missing_values[missing_values > 0])

"""## Preprocess data

### Subtask:
Handle any missing values and scale the features if necessary.

**Reasoning**:
Check for missing values in the features_df DataFrame and display the sum of missing values for each column.
"""

missing_values = features_df.isnull().sum()
print("Missing values per column:")
print(missing_values[missing_values > 0])

"""**Reasoning**:
Since there are no missing values, scale the features using StandardScaler and display the first few rows of the scaled DataFrame.


"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaled_features = scaler.fit_transform(features_df)
scaled_features_df = pd.DataFrame(scaled_features, columns=features_df.columns)
display(scaled_features_df.head())

"""**Reasoning**:
The previous output was truncated. Display the first few rows of the scaled_features_df DataFrame again to verify the scaling.


"""

display(scaled_features_df.head())

"""## Determine the optimal number of clusters

### Subtask:
Use a method like the elbow method or silhouette score to find the best number of clusters.

**Reasoning**:
Calculate silhouette scores for different numbers of clusters and plot them to find the optimal number of clusters.
"""

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt

silhouette_scores = []
for i in range(2, 11):
    kmeans = KMeans(n_clusters=i, random_state=42, n_init=10)
    kmeans.fit(scaled_features_df)
    score = silhouette_score(scaled_features_df, kmeans.labels_)
    silhouette_scores.append(score)

plt.figure(figsize=(10, 6))
plt.plot(range(2, 11), silhouette_scores, marker='o')
plt.title('Silhouette Scores vs. Number of Clusters')
plt.xlabel('Number of Clusters')
plt.ylabel('Silhouette Score')
plt.xticks(range(2, 11))
plt.grid(True)
plt.show()

"""## Apply k-means clustering

### Subtask:
Perform k-means clustering on the selected features.

**Reasoning**:
Performing k-means clustering requires importing the KMeans class and fitting the model to the scaled data. The instructions also require adding the cluster labels to both the original and scaled dataframes.
"""

from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
kmeans.fit(scaled_features_df)

df['cluster_label'] = kmeans.labels_
scaled_features_df['cluster_label'] = kmeans.labels_

display(df.head())
display(scaled_features_df.head())

"""## Analyze clusters

### Subtask:
Group the original data by cluster labels and analyze the characteristics of each cluster, including training needs, age, gender, and years of experience.

**Reasoning**:
Group the original DataFrame by cluster labels and calculate the mean for all columns to analyze the characteristics of each cluster.
"""

cluster_means = df.groupby('cluster_label').mean()
display(cluster_means)

"""## Visualize results (optional)

### Subtask:
Visualize the clusters using dimensionality reduction techniques like PCA.

**Reasoning**:
Visualize the clusters using PCA by creating a PCA object, fitting and transforming the scaled features, creating a new DataFrame with the principal components and cluster labels, and finally generating a scatter plot colored by cluster.
"""

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

pca = PCA(n_components=2)
principal_components = pca.fit_transform(scaled_features_df.drop('cluster_label', axis=1))

pca_df = pd.DataFrame(data = principal_components, columns = ['Principal Component 1', 'Principal Component 2'])
pca_df['cluster_label'] = scaled_features_df['cluster_label']

plt.figure(figsize=(10, 8))
for cluster_label in pca_df['cluster_label'].unique():
    plt.scatter(pca_df[pca_df['cluster_label'] == cluster_label]['Principal Component 1'],
                pca_df[pca_df['cluster_label'] == cluster_label]['Principal Component 2'],
                label=f'Cluster {cluster_label}')

plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA of Clusters')
plt.legend()
plt.show()

"""## Summary:

### Data Analysis Key Findings

*   Three distinct clusters were identified based on training needs.
*   Cluster 1 consists of younger individuals with less teaching experience who reported consistently higher average training needs across various categories.
*   Cluster 2 comprises older individuals with more teaching experience who reported consistently lower average training needs.
*   Cluster 0 represents a middle ground in terms of age, experience, and perceived training needs compared to the other two clusters.
*   There were no missing values in the selected training needs columns used for clustering.

### Insights or Next Steps

*   Tailor training programs specifically for Cluster 1 to address their higher perceived training needs.
*   Investigate the specific training needs reported by Cluster 0 to provide targeted development opportunities.

"""